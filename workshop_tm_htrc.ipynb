{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07eddfdf-21f1-4076-a663-2ac0595179fa",
   "metadata": {},
   "source": [
    "<h1>Text Mining with Hathitrust Feature Reader</h1>\n",
    "<h5>Created by Matt Steele, WVU</h5>\n",
    "<h5>Contact: <a href =\"https://directory.lib.wvu.edu/employee/210\" target=\"_blank\">https://directory.lib.wvu.edu/employee/210</a><h5>\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcbe54b-1898-4980-9204-81ea289a8a55",
   "metadata": {},
   "source": [
    "<div style =\"background-color:#f0f0f0; border:2px solid; width:75%; padding: 2.5%;\">\n",
    "    <h3>This workshop was developed using the following tutorials</h3>\n",
    "    <ul>\n",
    "        <li>Peter Organisciak and Boris Capitanu, \"Text Mining in Python through the HTRC Feature Reader,\" Programming Historian 5 (2016), https://doi.org/10.46430/phen0058.</li>\n",
    "        <li>HTRC Feature Reader Github Documentation Examples - https://github.com/htrc/htrc-feature-reader/tree/master/examples</li>\n",
    "    </ul>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9771320d-4c9f-4381-8378-f37b955097fc",
   "metadata": {},
   "source": [
    "<h2>About Hathitrust</h2>\n",
    "<p>The HathiTrust holds nearly 15 million digitized volumes from libraries around the world.</p>\n",
    "<ul><li><a href = \"https://databases.lib.wvu.edu/connect/1476909287\">Hathitrust</a></li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6eaf27b-b04a-4ab2-84ad-8f6cbdb00d39",
   "metadata": {},
   "source": [
    "<h2>About Hathitrust Research Center</h2>\n",
    "<p>Provides tools that enables computational analysis of works in the HathiTrust Digital Library (HTDL) to facilitate non-profit research and educational uses of the collection.</p>\n",
    "<ul><li><a href=\"https://analytics.hathitrust.org/\">Hathitrust Research Center</a></li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c293d6b-64e5-4064-bdae-c5688bb42db0",
   "metadata": {},
   "source": [
    "<h2>About Feature Reader</h2>\n",
    "<p>As part of its mission, the HTRC has released the Extracted Features (EF) dataset containing features derived for every page of 17 million ‘volumes’ (a generalized term referring to the different types of materials in the HathiTrust collection, of which books are the most prevalent type).</p>\n",
    "<p>A feature is a quantifiable marker of something measurable, a datum. A computer cannot understand the meaning of a sentence implicitly, but it can understand the counts of various words and word forms, or the presence or absence of stylistic markers, from which it can be trained to better understand text. Many text features are non-consumptive in that they don’t retain enough information to reconstruct the book text.</p>\n",
    "<p>The FeatureReader object is the interface for loading the dataset files and making sense of them. The files are originally formatted in a notation called JSON and compressed, which FeatureReader makes sense of and returns as Volume objects. A Volume is a representation of a single book or other work. This is where you access features about a work. Many features for a volume are collected from individual pages; to access Page information, you can use the Page object.</p>\n",
    "<ul><li><a href =\"http://htrc.github.io/htrc-feature-reader/htrc_features/feature_reader.m.html\">Feature Reader Documentation</a></li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823bbe46-891d-43b7-80fc-a7a183fa48fd",
   "metadata": {},
   "source": [
    "<h3>Installing Feature Reader</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baccb93c-045b-416d-9e02-19777f282578",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c htrc htrc-feature-reader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4492c57b-53c0-4181-a494-ba433b7a0bab",
   "metadata": {},
   "source": [
    "<h3>Calling Feature Reader Packages</h3>\n",
    "<ul>\n",
    "    <li><a href =\"http://htrc.github.io/htrc-feature-reader/htrc_features/feature_reader.m.html\">Feature Reader documentation</a></li>\n",
    "    <li><a href = \"https://wiki.htrc.illinois.edu/pages/viewpage.action?pageId=79069329\">Volume documentation</a></li>\n",
    "    <li><a href = \"http://htrc.github.io/htrc-feature-reader/htrc_features/utils.m.html\">utils documentation</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcd0cec-c2fa-45b8-be40-bb9749afc5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import htrc_features\n",
    "from htrc_features import Volume\n",
    "from htrc_features import utils\n",
    "from htrc_features import FeatureReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe73ee32-c068-42e4-88ef-9d8b9ffdeebc",
   "metadata": {},
   "source": [
    "<h3>Additional Packages</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8284be87-7abf-460a-9c47-68a897d6ac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a488e831-ebae-4275-83ff-27c4ee1326d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da99939-9a25-424b-89cd-8513fdfbf183",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6169d6-eb93-4d01-8aa9-d2d2b9ea0879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from wordcloud import STOPWORDS\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import ImageColorGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfab581-5428-474e-a632-a6bb26d82032",
   "metadata": {},
   "source": [
    "<h2>Scraping Volume information</h2>\n",
    "<p>A Volume contains information about the current work and access to the pages of the work. All the metadata fields from the HTRC JSON file are accessible as properties of the volume object.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd28690c-6a2f-41e5-b030-e62111b85373",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = Volume(\"lesson_files/data/mdp.39015003719849.json.bz2\")\n",
    "vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f8937f-c8fa-48e0-82c4-114df6a7ab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = Volume(\"mdp.39015003719849\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01acc02-fb3a-439d-a7fb-c65a7b55526f",
   "metadata": {},
   "source": [
    "<p>While the majority of the HTRC Extracted Features dataset is features, quantitative abstractions of a book’s written content, there is also a small amount of metadata included for each volume. Metadata attributes include:</p>\n",
    "<ul>\n",
    "    <li>Volume.title: The title of the volume. </li>\n",
    "    <li>Volume.id: A unique identifier for the volume in the HathiTrust and the HathiTrust Research Center. </li>\n",
    "    <li>Volume.year: The publishing date of the volume.</li>\n",
    "    <li>Volume.language: The classified language of the volume.</li>\n",
    "    <li>Volume.oclc: The OCLC control number(s). The volume id can be used to pull more information from other sources.</li> \n",
    "</ul>        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a5f1de-35c5-433d-a5e3-44bd3fa7d898",
   "metadata": {},
   "source": [
    "<h3>TAB button: Viewing options</h3>\n",
    "<p>Use the TAB button after your named variable <vol.> to see what options are available to view.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8b121a-5aa4-433d-8491-8411362b0868",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vol.title, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d571d862-6fa1-43c1-8c2e-f8641ea4e539",
   "metadata": {},
   "source": [
    "<h3>Accessing the Volume</h3>\n",
    "<p>The scanned copy of the book can be found from the HathiTrust Digital Library, when available, by accessing http://hdl.handle.net/2027/{VOLUME ID}. In the feature reader, this url is retrieved by calling vol.handle_url:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf3b0c5-6f9d-4b75-998c-8c3d5c2d7efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vol.handle_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c60033-07c1-4d28-b2ed-87eac82681e4",
   "metadata": {},
   "source": [
    "<h3>Viewing Tokens per Page</h3>\n",
    "<p>This is the Extracted Features dataset, so the features are easily accessible. To most popular is token counts, which are returned as a Pandas DataFrame. Listed in the table are page numbers and the count of words on each page.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db85770-b9cc-4e0c-a4e4-604b62467215",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count = vol.tokens_per_page()\n",
    "token_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fee02b8-f10e-4644-afde-bbcad4dd0c9e",
   "metadata": {},
   "source": [
    "<h3>Plot Words per Page</h3>\n",
    "<p>With only two dimensions, it is trivial to plot the number of words per page. The table structure holding the data has a plot method for data graphics. Without extra arguments, tokens.plot() will assume that you want a line chart with the page on the x-axis and word count on the y-axis.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de521115-43fa-498e-ba96-aa74cc257ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "token_count.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98266b12-f310-4f8d-98a9-632ab630e80b",
   "metadata": {},
   "source": [
    "<h3>Viewing Tokens in the Volume</h3>\n",
    "<p>The Extracted Features dataset also provides token counts with much more granularity: for every part of speech (e.g. noun, verb) of every occurring capitalization of every word of every section (i.e. header, footer, body) of every page of the volume.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc230433-29f4-40b8-bd63-0131e3f35ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = vol.tokenlist()\n",
    "tl.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97c9423-3f6d-4cd0-b4c5-e1e634977e13",
   "metadata": {},
   "source": [
    "<p>The columns in bold are an index. Unlike the typical one-dimensional index seen before, here there are four dimensions to the index: page, section, token, and pos. This row says that for the nth page, in the n section (i.e. ignoring any words in the header or footer), the word ‘n’ occurs n time as an plural noun. The part-of-speech tag (POS) follows the Penn Treebank definitions.</p>\n",
    "<ul><li><a href = \"https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\">Penn Treebank definition</a></li></ul>\n",
    "<p> The HTRC Feature Reader refers to “pages” as the nth scanned image of the volume, not the actual number printed on the page. This is why “page 1” for this example is the cover.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f08c91-2f27-4082-9762-5604f0d30e81",
   "metadata": {},
   "source": [
    "<h3>Cleaning the Text</h3>\n",
    "<p>Tokenlists can be retrieved with arguments that combine information by certain dimensions, such as case, pos, or page. For example, case=False specified that “Jaguar” and “jaguar” should be counted together. You may also notice that, by default, only ‘body’ is returned, a default that can be overridden.</p>\n",
    "<p>Let's see what happens if we play with the various commands:</p>\n",
    "<ul>\n",
    "    <li>vol.tokenlist(case=False)</li>\n",
    "    <li>vol.tokenlist(pos=False)</li>\n",
    "    <li>vol.tokenlist(pages=False, case=False, pos=False)</li>\n",
    "    <li>vol.tokenlist(section='header')</li> \n",
    "    <li>vol.tokenlist(section='group')</li>\n",
    "    </ul>\n",
    "    \n",
    "<p>Details for these arguments are available in the code documentation for the Feature Reader.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bbba73-2b0a-4068-9a70-532c043090a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run test here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6142e1d7-58e2-4b1e-ad4b-1d20069a6dd8",
   "metadata": {},
   "source": [
    "<h3>Selecting Subsets of a DataFrame by a Condition</h3>\n",
    "<p>Because EF creates dataframes of the information in a volume, we can subset individual rows of a DataFrame. This allows us to provide a series of True/False values to the DataFrame, formatted in square brackets. When True, the DataFrame returns that row; when False, the row is excluded from what is returned.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad40f6a8-3de4-40b6-8c08-a4dbc1730fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_simple = vol.tokenlist(pos=False, pages=False, case=False)\n",
    "# .sample(5) returns five random words from the full result\n",
    "tl_simple.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1b89de-7781-447a-9e5b-dbb38be30415",
   "metadata": {},
   "source": [
    "Let's see what words are occur greater than 100 times in the volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0386b975-a508-4760-bfb2-da939fa76265",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_simple[\"count\"] > 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98c00a0-a495-4c7d-b225-acdbda0b63fd",
   "metadata": {},
   "source": [
    "Let's use the same code to create a new dataframe that includes only the words that occur over 100 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8dd413-fc41-43b1-a6bd-5edfee56b02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = tl_simple['count'] > 100\n",
    "tl_simple[matches].sample(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756c6f3c-b621-456a-a105-92d2e684861b",
   "metadata": {},
   "source": [
    "<h3>Section Features</h3>\n",
    "<p>Explore the format of pages with the section_features function</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6736795-7982-4c7b-bcaa-1ee400207df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_df = vol.section_features()\n",
    "section_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aab6ded-6bde-4fec-b965-2844efd6ad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_df.plot(y=[\"sentenceCount\", \"tokenCount\", \"lineCount\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dceb7f5-74d4-4c31-846b-688ba03ff744",
   "metadata": {},
   "source": [
    "<h3>Chunking</h3>\n",
    "<p>The process of splitting text into smaller or larger pieces before analysis. May be divided by paragraph, chapter, or a chosen number of words. If you're working in an instance where you hope to have comparably sized document units, you can use 'chunking' to roll pages into chunks that aim for a specific length.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9355803-12a9-4596-8133-9aaa8929a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_chunk = vol.tokenlist(chunk=True, chunk_target=10000)\n",
    "print(by_chunk.sample(10))\n",
    "# Count words per chunk\n",
    "by_chunk.groupby(level='chunk').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67df1d52-524e-4b50-95af-bc11ee4f2057",
   "metadata": {},
   "source": [
    "<h3>Read multiple files</h3>\n",
    "<p>We can use the package glob to load multiple files into python for analysis.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c43f62-a098-426f-8d8c-1b03e359cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "paths = glob.glob('lesson_files/data/*bz2', recursive=True)\n",
    "fr = FeatureReader(paths)\n",
    "for vol in fr.volumes():\n",
    "    print(vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8a0ded-d037-4af8-8ed7-0ff5db6aa4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = next(fr.volumes())\n",
    "tl = vol.tokenlist(pages=False)\n",
    "tl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf8c631-5811-4a72-8edc-1722cccecf54",
   "metadata": {},
   "source": [
    "<h3>Using Pandas Functions to Filter to Only Proper Nouns</h3>\n",
    "<p>We'll set the first volume of the FeatureReader to vol and return a tokenlist, without page-level information.</p>\n",
    "<p>I'm interested in the occurance of words across years, so we'll add a date column and absorb it into the MultiIndex as a new level. At the same time, we'll drop the section level, since it's all redundant information. You can read about Pandas MultIndexes in the Pandas documentation.</p>\n",
    "<ul><li><a href = \"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.MultiIndex.html\">Pandas MultIndexes in the Pandas documentation</a></li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62924f24-61d5-4ee1-b532-b779566b47c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'section', which is level 0 of the MultiIndex\n",
    "tl.index = tl.index.droplevel(0)\n",
    "# Add date column, convert to index level, and reorder levels\n",
    "tl['date'] = vol.year\n",
    "tl = tl.set_index('date', append=True).reorder_levels(['date', 'token', 'pos'])\n",
    "tl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200227cb-3459-4e02-ab20-ae27324cbcee",
   "metadata": {},
   "source": [
    "<p>The Extracted Features dataset using the part-of-speech tags from the Penn Treebank. In Penn, proper nouns are labelled NNP and plural proper nouns are labelled NNPS.</p>\n",
    "<ul><li><a href = \"https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\">Penn Treebank definition</a></li></ul>\n",
    "\n",
    "<p>To get all the proper nouns, we'll 'slice' all the columns that have NNP or NNPS as the part-of-speech (POS) value.</p>\n",
    "\n",
    "<p>Slicing involves using the .loc[]. Note that we ask for idx[:,:,('NNP', 'NNPS')] below. This is asking, in order, for<p>\n",
    "\n",
    "<ol><li>any matching date</li>\n",
    "    <li>any matching token</li>\n",
    "    <li>only pos rows that match NNP or NNPS</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73965f86-9f27-4874-9e27-ffff3a6c71c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "proper_nouns = tl.loc[idx[:,:,('NNP', 'NNPS')],]\n",
    "# Show only proper nouns that occur more than once\n",
    "proper_nouns[proper_nouns['count'] > 1].sort_values('count', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca344a12-8ed3-4e06-ad71-45b5a2eaac0a",
   "metadata": {},
   "source": [
    "<h3>Retrieve Proper Nouns over Multiple Volumes</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b87f2d-4c2d-4e8a-be0a-8efb07e991d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "\n",
    "def get_proper_nouns(vol):\n",
    "    tl = vol.tokenlist(pages=False)\n",
    "    tl.index = tl.index.droplevel(0)\n",
    "    tl['date'] = vol.year\n",
    "    tl = tl.set_index('date', append=True).reorder_levels(['date', 'token', 'pos'])\n",
    "    try:\n",
    "        proper_nouns = tl.loc[idx[:,:,('NNP', 'NNPS')],]\n",
    "        proper_nouns.index = proper_nouns.index.droplevel(2)\n",
    "        return proper_nouns[proper_nouns['count'] > 1]\n",
    "    except:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cac999-dc11-4f61-a5b8-a4d05df92538",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnp_dfs = []\n",
    "for vol in fr.volumes():\n",
    "    nnp_dfs.append(get_proper_nouns(vol))\n",
    "all_nnp = pd.concat(nnp_dfs)\n",
    "del nnp_dfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad949774-5bde-4379-9823-157977bbd1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nnp.sort_values(['count', 'date'], ascending=False)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e491190d-70c8-4a77-ac2f-2eca6bbe5214",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnp_flat = all_nnp.reset_index()\n",
    "nnp_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d4a20f-2a32-4bb5-a279-613042a4bd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(i for i in nnp_flat.token)\n",
    "stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(stopwords=stopwords, background_color=\"black\", max_words=200).generate(text)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(wordcloud, interpolation='gaussian')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee7dac8-2996-4848-a985-c95696cf39a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>Sentiment Analysis using AFINN</h3>\n",
    "<p>Load up \"Operating Manual for Spaceship\"</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba01a4bb-df7f-4f77-946e-b4b3777b99a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = Volume(\"lesson_files/data/mdp.39015003719849.json.bz2\")\n",
    "vol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc7cc66-a8d2-41d2-bda9-e9d543f655a7",
   "metadata": {},
   "source": [
    "<p>Load Up the AFINN dataframe</p>\n",
    "<ul><li><a href =\"http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010\">Download AFINN Dataframe</a></li></ul> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f320c6f6-ea2a-49eb-a8d3-25959c571a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "afinn = pd.read_csv(\"AFINN-111.txt\", sep='\\t', names=['token','valence'])\n",
    "afinn[::600]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2237d0-9aaa-4660-b3f4-94121d081ae2",
   "metadata": {},
   "source": [
    "<h3>Get the tokenlist</h3>\n",
    "<p>Because AFINN is case-insensitive and doesn't take part-of-speech into account, we don't need that information. We won't be doing any slicing by the multi-index and hope to group by page numbers, so we'll also convert the index to columns with reset_index() and drop the unnecessary section column. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e7bff1-1d97-4bbd-b0e8-a65f2ec6618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = vol.tokenlist(pos=False, case=False)\\\n",
    "        .reset_index().drop(['section'], axis=1)\n",
    "tl.columns = ['page', 'token', 'count']\n",
    "tl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5982ae-068d-40f0-bf5a-1b2b8e4f25ed",
   "metadata": {},
   "source": [
    "<h3>Clean up the Data</h3>\n",
    "<p>Use the group by function to group the tokens by page and filter out low used words. We only want to classify full pages, so let's see the average page length and exclude pages that have less words than 80% of the average.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe11256-07f5-4f70-b2d4-e0613c80cc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanpages = tl.groupby('page').sum().mean()['count']\n",
    "tl2 = tl.groupby('page').filter(lambda x: x['count'].sum() > meanpages*0.8)\n",
    "tl2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be7d827-05a5-4688-a76d-257039d6ce40",
   "metadata": {},
   "source": [
    "<h3>Merge the Dataframes</h3>\n",
    "<p>Merge the AFINN to the TL and add the sentiment scores for all words that have a valence assigned in AFINN.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11e788d-fea5-4c89-bf2a-37555a87a32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl3 = pd.merge(tl, afinn)\n",
    "# Account for multiple occurrances of the same word\n",
    "tl3['sentiment_weight'] = tl3['count'] * tl3['valence']\n",
    "perpage = tl3.drop(['count', 'valence'], axis=1).groupby(['page'], as_index=False).sum()\n",
    "perpage[:100:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc875f0-5218-462e-b0ae-7598ccf509c1",
   "metadata": {},
   "source": [
    "<h3>Plot the Sentiment</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a36b07-2532-4e43-b1e9-bd0ba47f5274",
   "metadata": {},
   "outputs": [],
   "source": [
    "perpage.plot(x='page',y=['sentiment_weight'], title=\"Sentiment through Operating Manual for Planet Earth by Page\", xlabel=\"Page\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
